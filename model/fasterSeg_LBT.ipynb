{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siz075/ECE269_final_project/model/tools/utils/darts_utils.py:179: UserWarning: TensorRT (or pycuda) is not installed. compute_latency_ms_tensorrt() cannot be used.\n",
      "  warnings.warn(\"TensorRT (or pycuda) is not installed. compute_latency_ms_tensorrt() cannot be used.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from thop import profile\n",
    "from random import shuffle\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from config_search import config\n",
    "from dataloader import get_train_loader, get_valid_loader\n",
    "from tools.datasets import Cityscapes\n",
    "from architect_lbt import Architect\n",
    "from model_search import Network_Multi_Path as Network\n",
    "from model_seg import Network_Multi_Path_Infer\n",
    "from utils.darts_utils import create_exp_dir, save, plot_op, plot_path_width, objective_acc_lat\n",
    "from utils.init_func import init_weight\n",
    "from eval import SegEvaluator\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(config.num_classes, config.layers, Fch=config.Fch, width_mult_list=config.width_mult_list, prun_modes=['max'], stem_head_width=config.stem_head_width)\n",
    "# model = model.cuda()\n",
    "student = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=False, progress=True, num_classes=config.num_classes, aux_loss=None)\n",
    "# student = student.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using downsampling: 2\n",
      "Found 1487 images\n",
      "using downsampling: 2\n",
      "Found 1488 images\n",
      "using downsampling: 2\n",
      "Found 500 images\n",
      "using downsampling: 2\n",
      "Found 500 images\n"
     ]
    }
   ],
   "source": [
    "architect = Architect(model, student)\n",
    "\n",
    "init_weight(model, nn.init.kaiming_normal_, nn.BatchNorm2d, config.bn_eps, config.bn_momentum, mode='fan_in', nonlinearity='relu')\n",
    "data_setting = {'img_root': config.img_root_folder,\n",
    "                    'gt_root': config.gt_root_folder,\n",
    "                    'train_source': config.train_source,\n",
    "                    'eval_source': config.eval_source,\n",
    "                    'down_sampling': config.down_sampling}\n",
    "index_select = list(range(config.num_train_imgs))\n",
    "shuffle(index_select)  # shuffle to make sure balanced dataset split\n",
    "train_loader_model = get_train_loader(config, Cityscapes, portion=config.train_portion, index_select=index_select)\n",
    "train_loader_arch = get_train_loader(config, Cityscapes, portion=config.train_portion-1, index_select=index_select)\n",
    "evaluator = SegEvaluator(Cityscapes(data_setting, 'val', None), config.num_classes, config.image_mean,\n",
    "                             config.image_std, model, config.eval_scale_array, config.eval_flip, 0, config=config,\n",
    "                             verbose=False, save_path=None, show_image=False)\n",
    "valid_loader = get_valid_loader(config, Cityscapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = config.lr\n",
    "parameters = []\n",
    "parameters += list(model.stem.parameters())\n",
    "parameters += list(model.cells.parameters())\n",
    "parameters += list(model.refine32.parameters())\n",
    "parameters += list(model.refine16.parameters())\n",
    "parameters += list(model.head0.parameters())\n",
    "parameters += list(model.head1.parameters())\n",
    "parameters += list(model.head2.parameters())\n",
    "parameters += list(model.head02.parameters())\n",
    "parameters += list(model.head12.parameters())\n",
    "optimizer = torch.optim.SGD(\n",
    "    parameters,\n",
    "    lr=base_lr,\n",
    "    momentum=config.momentum,\n",
    "    weight_decay=config.weight_decay)\n",
    "optimizer_stud = torch.optim.SGD(student.parameters(),lr=base_lr,\n",
    "    momentum=config.momentum,\n",
    "    weight_decay=config.weight_decay)\n",
    "# lr policy ##############################\n",
    "lr_policy = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.978)\n",
    "lr_policy_stud = torch.optim.lr_scheduler.ExponentialLR(optimizer_stud, 0.978)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = iter(train_loader_model).next()\n",
    "imgs = minibatch['data']\n",
    "labels = minibatch['label']\n",
    "minibatch_val = iter(train_loader_arch).next()\n",
    "imgs_val = minibatch_val['data']\n",
    "labels_val = minibatch_val['label']\n",
    "minibatch_unlabeled = iter(valid_loader).next()\n",
    "imgs_unlabeled = minibatch_unlabeled['data']\n",
    "labels_unlabeled = minibatch_unlabeled['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "out = student(imgs)\n",
    "logits = nn.functional.interpolate(out['out'],size=(224//8,448//8))\n",
    "loss = criterion(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_model = model._loss(imgs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architect.step(imgs, labels, imgs_val, labels_val, imgs_unlabeled, base_lr, optimizer, unrolled=True)\n",
    "architect.step1(imgs, labels, imgs_val, labels_val, imgs_unlabeled, base_lr, optimizer, optimizer_stud, unrolled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-latest",
   "language": "python",
   "name": "ml-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
